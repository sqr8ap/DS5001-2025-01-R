








import pandas as pd
import numpy as np
import seaborn as sns
import nltk
import re





import configparser


config = configparser.ConfigParser()
config.read("../../../env.ini")
data_dir = config['DEFAULT']['data_home']
output_dir = config['DEFAULT']['output_dir']








src_file = f"{data_dir}/gutenberg/pg161.txt"
OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']





LINES = pd.DataFrame(open(src_file, 'r', encoding='utf-8-sig').readlines(), columns=['line_str'])
LINES.index.name = 'line_num'
LINES.line_str = LINES.line_str.str.replace(r'\n+', ' ', regex=True).str.strip()


LINES.sample(20)





clip_pats = [
    r"^\(1811\)", 
    r"^THE END"
]

line_a = 0
line_b = len(LINES)
try:
    pat_a = LINES.line_str.str.match(clip_pats[0])
    line_a = LINES.loc[pat_a].index[0] + 1
except:
    print("pat_a not found")

try:
    pat_b = LINES.line_str.str.match(clip_pats[1])
    line_b = LINES.loc[pat_b].index[0] - 1
except:
    print("pat_b not found")

LINES = LINES.loc[line_a : line_b]


LINES.head()


LINES.tail()





chap_pat = r"^\s*(CHAPTER)\s+\d+"
chap_lines = LINES.line_str.str.match(chap_pat, case=False)
LINES.loc[chap_lines, 'chap_num'] = [i+1 for i in range(LINES.loc[chap_lines].shape[0])]
LINES.chap_num = LINES.chap_num.ffill()
LINES = LINES.dropna(subset=['chap_num']) # Remove everything before Chapter 1
LINES = LINES.loc[~chap_lines] # Remove chapter heading lines; their work is done
LINES.chap_num = LINES.chap_num.astype('int') # Convert chap_num from float to int


CHAPS = LINES.groupby(OHCO[:1]).line_str\
    .apply(lambda x: '\n'.join(x))\
    .str.strip()\
    .to_frame('chap_str')


CHAPS.head()





para_pat = r'\n\n+'
PARAS = CHAPS['chap_str'].str.split(para_pat, expand=True).stack()\
    .to_frame('para_str').sort_index()
PARAS.index.names = OHCO[:2]
PARAS['para_str'] = PARAS['para_str'].str.replace(r'\n', ' ', regex=True)
PARAS['para_str'] = PARAS['para_str'].str.strip()
PARAS = PARAS[~PARAS['para_str'].str.match(r'^\s*$')] # Remove empty paragraphs


PARAS.head()














nltk_resources = [
    'tokenizers/punkt', 
    'taggers/averaged_perceptron_tagger', 
    'corpora/stopwords', 
    'help/tagsets'
]


for resource in nltk_resources:
    try:
        nltk.data.find(resource)
    except IndexError:
        nltk.download(resource)








sample_para = PARAS.iloc[1].para_str


sample_para


print('\n\n'.join(nltk.sent_tokenize(sample_para)))





SENTS = PARAS.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\
        .stack()\
        .to_frame('sent_str')
SENTS.index.names = OHCO[:3]


SENTS





sample_sent = SENTS.iloc[3].sent_str


sample_sent


sample_tokens = nltk.word_tokenize(sample_sent)


print(sample_tokens)





# nltk.pos_tag?


sample_tagged_tokens = nltk.pos_tag(nltk.word_tokenize(sample_sent), tagset='universal')


print(sample_tagged_tokens)





keep_whitespace = True


# nltk.WhitespaceTokenizer?


# nltk.word_tokenize?


if keep_whitespace:
    # Return a tokenized copy of text
    # using NLTK's recommended word tokenizer.
    TOKENS = SENTS.sent_str\
            .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\
            .stack()\
            .to_frame('pos_tuple')
else:
    # Tokenize a string on whitespace (space, tab, newline).
    # In general, users should use the string ``split()`` method instead.
    # Returns fewer tokens.
    TOKENS = SENTS.sent_str\
            .apply(lambda x: pd.Series(nltk.pos_tag(nltk.WhitespaceTokenizer().tokenize(x))))\
            .stack()\
            .to_frame('pos_tuple')


TOKENS.index.names = OHCO


TOKENS


TOKENS['pos'] = TOKENS.pos_tuple.apply(lambda x: x[1])
TOKENS['token_str'] = TOKENS.pos_tuple.apply(lambda x: x[0])
TOKENS['term_str'] = TOKENS.token_str.str.lower().str.replace(r"\W+", "", regex=True)


TOKENS





TOKENS['pos_group'] = TOKENS.pos.str[:2]


TOKENS.head()








VOCAB = TOKENS.term_str.value_counts().to_frame('n')
VOCAB.index.name = 'term_str'
VOCAB['p'] = VOCAB.n / VOCAB.n.sum()
VOCAB['i'] = -np.log2(VOCAB.p)
VOCAB['n_chars'] = VOCAB.index.str.len()


VOCAB





TOKENS[['term_str','pos_group']].value_counts().sort_index().loc['love']


TOKENS[['term_str','pos']].value_counts().sort_index().loc['love']


VOCAB['max_pos_group'] = TOKENS[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)


VOCAB['max_pos'] = TOKENS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)


VOCAB











tags_csv = [(line.split()[0], ' '.join(line.split()[1:])) 
            for line in open(f'{data_dir}/misc/upenn_tagset.txt', 'r').readlines()]


POS = pd.DataFrame(tags_csv)
POS.columns = ['pos_code','pos_def']
POS = POS.set_index('pos_code')
POS['n'] = TOKENS.pos.value_counts()
POS['n'] = POS['n'].fillna(0).astype('int')
POS['group'] = POS.index.str[:2]
POS['punc'] = POS.index.str.match(r"^\W")


POS


POS[POS.punc].index.to_list()


POS.groupby('group').n.sum().sort_values().plot.bar(figsize=(10,5), rot=0);





VOCAB.sort_values('n', ascending=False).head(10)





TPM1 = TOKENS[['term_str','pos_group']].value_counts().unstack()


TPM1


TPM1.count(1)


VOCAB['n_pos_group'] = TPM1.count(1)


TPM2 = TOKENS[['term_str','pos']].value_counts().unstack()


TPM2


VOCAB['n_pos'] = TPM2.count(1)


VOCAB.sort_values('n_pos', ascending=False).head(10)


# VOCAB.plot.scatter('n_pos', 'n_pos_group');








sw = pd.DataFrame({'stop': 1}, index=nltk.corpus.stopwords.words('english'))
sw.index.name='term_str'


sw.head()


if 'stop' not in VOCAB.columns:
    VOCAB = VOCAB.join(sw)
    VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')


VOCAB





from nltk.stem.porter import PorterStemmer
stemmer = PorterStemmer()
VOCAB['p_stem'] = VOCAB.apply(lambda x: stemmer.stem(x.name), 1)


VOCAB





VOCAB.p_stem.value_counts().head()


VOCAB[VOCAB.p_stem == 'respect']


VOCAB[VOCAB.p_stem == 'observ']



