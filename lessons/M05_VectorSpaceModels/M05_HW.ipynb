{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8944ef-6c6b-4029-815a-ee2d4ebc711c",
   "metadata": {},
   "source": [
    "# M05 Homework\n",
    "- Name: Sam Remmey\n",
    "- Net ID: sqr8ap\n",
    "- URL of this file in GitHub: https://github.com/sqr8ap/DS5001-2025-01-R/blob/m05/lessons/M05_VectorSpaceModels/M05_HW.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e4ecf9f-2b43-4970-83a6-a9c4dca29699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be1b0191-0836-4727-a70c-b26c987cbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "575f2664-a925-4f5b-92bb-4a8ec078871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home'] \n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "data_prefix = 'austen-melville'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdb042d0-04ff-4cd2-8483-c5db4221af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_id', 'para_num', 'sent_num', 'token_num']\n",
    "bags = dict(\n",
    "    SENTS = OHCO[:4],\n",
    "    PARAS = OHCO[:3],\n",
    "    CHAPS = OHCO[:2],\n",
    "    BOOKS = OHCO[:1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5edd981-fb5d-4428-a025-5fe9504e5900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SENTS': ['book_id', 'chap_id', 'para_num', 'sent_num'],\n",
       " 'PARAS': ['book_id', 'chap_id', 'para_num'],\n",
       " 'CHAPS': ['book_id', 'chap_id'],\n",
       " 'BOOKS': ['book_id']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "add99103-59c9-4363-9ca0-e8b5888eccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = 'CHAPS'\n",
    "# bag = 'BOOKS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7321724-b955-4dd7-b072-e160ab210e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = pd.read_csv(f\"{output_dir}/{data_prefix}-LIB.csv\").set_index('book_id')\n",
    "TOKEN = pd.read_csv(f'{output_dir}/{data_prefix}-CORPUS.csv').set_index(OHCO).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "817c0f37-68ae-4c92-928a-6ae1045798b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id\n",
       "105       83613\n",
       "121       77586\n",
       "141      160366\n",
       "158      160884\n",
       "161      119858\n",
       "946       23115\n",
       "1212      33241\n",
       "1342     122089\n",
       "1900     108015\n",
       "2701     215461\n",
       "4045     102347\n",
       "8118     119230\n",
       "10712    143251\n",
       "13720     96874\n",
       "13721    102078\n",
       "15422     65510\n",
       "15859     75232\n",
       "21816     95169\n",
       "34970    155024\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN.reset_index().book_id.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833c4d3-3d9e-4df4-b94d-85ec7b5b78c8",
   "metadata": {},
   "source": [
    "## Write a function to create BOW from TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c253ee36-73c4-45ea-8c33-6ff5897de093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bow(TOKENS, OHCO_LEVEL='CHAPS'):\n",
    "    '''\n",
    "    This function takes a tokens table and a choice of bag and returns a BOW representation in the form of a document-term count matrix. \n",
    "\n",
    "    Parameters\n",
    "    TOKENS: tokens table; a dataframe\n",
    "    OCHO_LEVEL: choice of bag; a string (either 'BOOKS', 'CHAPS', 'PARAS' or 'SENTS'); defaults to 'CHAPS'\n",
    "\n",
    "    Returns\n",
    "    DTCM: document-term count matrix\n",
    "    '''\n",
    "    \n",
    "    bags = dict(\n",
    "        SENTS = OHCO[:4],\n",
    "        PARAS = OHCO[:3],\n",
    "        CHAPS = OHCO[:2],\n",
    "        BOOKS = OHCO[:1])\n",
    "    \n",
    "    BOW = TOKENS.groupby(bags[OHCO_LEVEL]+['term_str']).term_str.count().to_frame('n')\n",
    "    DTCM = BOW.n.unstack(fill_value=0)\n",
    "\n",
    "    return DTCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8852fc9a-bf4c-4a74-a6c7-574351b15ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dtcm = gen_bow(TOKEN, 'CHAPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14072f1a-ec94-476d-8eb3-9421799c3393",
   "metadata": {},
   "source": [
    "## Write a function to return TFIDF values for a given BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "555c83a9-ff3d-421d-b4d4-d87a424feb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_tfidf(DTCM, TF_METHOD='sum'):\n",
    "    '''\n",
    "    This function takes a BOW table (DTCM) and type of tf metric and returns the TFIDF values for the BOW. \n",
    "\n",
    "    Parameters\n",
    "    DTCM: BOW table; a dataframe\n",
    "    TF_METHOD: a string; either 'sum', 'max', 'log', 'raw', 'double_norm' or 'binary'; defaults to 'sum'\n",
    "\n",
    "    Returns\n",
    "    TFIDF: a dataframe\n",
    "    '''\n",
    "\n",
    "    tf_norm_k = 0.5\n",
    "    idf_method = 'standard'\n",
    "    gradient_cmap = 'YlGnBu'\n",
    "    tf = {\n",
    "        'sum': (DTCM.T / DTCM.T.sum()).T,\n",
    "        'max': (DTCM.T / DTCM.T.max()).T,\n",
    "        'log': (np.log2(1 + DTCM.T)).T,\n",
    "        'raw':  DTCM,\n",
    "        'double_norm': (DTCM.T / DTCM.T.max()).T,\n",
    "        'binary': DTCM.T.astype('bool').astype('int').T}\n",
    "\n",
    "    TF = tf[TF_METHOD]\n",
    "\n",
    "    DF = DTCM.astype('bool').sum() \n",
    "\n",
    "    N = DTCM.shape[0]   \n",
    "    \n",
    "    IDF = np.log2(N / DF)\n",
    "\n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    return TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "005dbbde-9fb6-446e-b2d6-2a8cb52a1d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>10000</th>\n",
       "      <th>10000000</th>\n",
       "      <th>10440</th>\n",
       "      <th>10800</th>\n",
       "      <th>10th</th>\n",
       "      <th>...</th>\n",
       "      <th>zoroaster</th>\n",
       "      <th>zozo</th>\n",
       "      <th>zuma</th>\n",
       "      <th>zur</th>\n",
       "      <th>à</th>\n",
       "      <th>æneas</th>\n",
       "      <th>æniad</th>\n",
       "      <th>æson</th>\n",
       "      <th>æsops</th>\n",
       "      <th>ł20000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">105</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term_str           0         1   10  100  1000  10000  10000000  10440  10800  \\\n",
       "book_id chap_id                                                                 \n",
       "105     1        0.0  0.005048  0.0  0.0   0.0    0.0       0.0    0.0    0.0   \n",
       "        2        0.0  0.000000  0.0  0.0   0.0    0.0       0.0    0.0    0.0   \n",
       "        3        0.0  0.000000  0.0  0.0   0.0    0.0       0.0    0.0    0.0   \n",
       "        4        0.0  0.000000  0.0  0.0   0.0    0.0       0.0    0.0    0.0   \n",
       "        5        0.0  0.000000  0.0  0.0   0.0    0.0       0.0    0.0    0.0   \n",
       "\n",
       "term_str         10th  ...  zoroaster  zozo  zuma  zur    à  æneas  æniad  \\\n",
       "book_id chap_id        ...                                                  \n",
       "105     1         0.0  ...        0.0   0.0   0.0  0.0  0.0    0.0    0.0   \n",
       "        2         0.0  ...        0.0   0.0   0.0  0.0  0.0    0.0    0.0   \n",
       "        3         0.0  ...        0.0   0.0   0.0  0.0  0.0    0.0    0.0   \n",
       "        4         0.0  ...        0.0   0.0   0.0  0.0  0.0    0.0    0.0   \n",
       "        5         0.0  ...        0.0   0.0   0.0  0.0  0.0    0.0    0.0   \n",
       "\n",
       "term_str         æson  æsops  ł20000  \n",
       "book_id chap_id                       \n",
       "105     1         0.0    0.0     0.0  \n",
       "        2         0.0    0.0     0.0  \n",
       "        3         0.0    0.0     0.0  \n",
       "        4         0.0    0.0     0.0  \n",
       "        5         0.0    0.0     0.0  \n",
       "\n",
       "[5 rows x 40281 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tfidf(my_dtcm).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28659f46-33b6-4c9b-a65b-c3686e21c287",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112bfa6-34aa-4632-89a1-a075431dd62b",
   "metadata": {},
   "source": [
    "#### 1. Show the function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a218d491-c6a6-42b8-9375-ea910094d841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes a tokens table and a choice of bag and returns a BOW representation in the form of a document-term count matrix. \n",
      "\n",
      "    Parameters\n",
      "    TOKENS: tokens table; a dataframe\n",
      "    OCHO_LEVEL: choice of bag; a string (either 'BOOKS', 'CHAPS', 'PARAS' or 'SENTS'); defaults to 'CHAPS'\n",
      "\n",
      "    Returns\n",
      "    DTCM: document-term count matrix\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(gen_bow.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "533434e8-3f2f-4ed7-859a-d15e325e5adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    This function takes a BOW table (DTCM) and type of tf metric and returns the TFIDF values for the BOW. \n",
      "\n",
      "    Parameters\n",
      "    DTCM: BOW table; a dataframe\n",
      "    TF_METHOD: a string; either 'sum', 'max', 'log', 'raw', 'double_norm' or 'binary'; defaults to 'sum'\n",
      "\n",
      "    Returns\n",
      "    TFIDF: a dataframe\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(gen_tfidf.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aaab43-8f73-43ec-a8a3-1af1290e12db",
   "metadata": {},
   "source": [
    "See above for implementations of both functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f242a66-4386-41c8-aabb-610849d8d6f9",
   "metadata": {},
   "source": [
    "#### 2. What are the top 20 words in the corpus by TFIDF mean using the `max` count method and `book` as the bag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "59d5d5ae-4b89-4c39-8d8b-14f40897a6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elinor</th>\n",
       "      <td>0.033840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pierre</th>\n",
       "      <td>0.030911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vernon</th>\n",
       "      <td>0.025980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marianne</th>\n",
       "      <td>0.021347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emma</th>\n",
       "      <td>0.021164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>darcy</th>\n",
       "      <td>0.019302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reginald</th>\n",
       "      <td>0.018486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babbalanja</th>\n",
       "      <td>0.018252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catherine</th>\n",
       "      <td>0.018238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frederica</th>\n",
       "      <td>0.017986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crawford</th>\n",
       "      <td>0.017749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fanny</th>\n",
       "      <td>0.017167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elliot</th>\n",
       "      <td>0.017053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weston</th>\n",
       "      <td>0.016591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>0.015986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>0.015428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knightley</th>\n",
       "      <td>0.015184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tilney</th>\n",
       "      <td>0.013815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elton</th>\n",
       "      <td>0.013648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bingley</th>\n",
       "      <td>0.013264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfidf\n",
       "term_str            \n",
       "elinor      0.033840\n",
       "pierre      0.030911\n",
       "vernon      0.025980\n",
       "marianne    0.021347\n",
       "emma        0.021164\n",
       "darcy       0.019302\n",
       "reginald    0.018486\n",
       "babbalanja  0.018252\n",
       "catherine   0.018238\n",
       "frederica   0.017986\n",
       "crawford    0.017749\n",
       "fanny       0.017167\n",
       "elliot      0.017053\n",
       "weston      0.016591\n",
       "media       0.015986\n",
       "israel      0.015428\n",
       "knightley   0.015184\n",
       "tilney      0.013815\n",
       "elton       0.013648\n",
       "bingley     0.013264"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtcm_books = gen_bow(TOKEN, 'BOOKS')\n",
    "tfidf_books = gen_tfidf(dtcm_books, TF_METHOD='max')\n",
    "\n",
    "\n",
    "tfidf_mean = tfidf_books.mean(axis=0)  # Mean TF-IDF per term\n",
    "top20_book = pd.DataFrame(tfidf_mean.sort_values(ascending=False).head(20), columns = ['tfidf'])\n",
    "top20_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5afe6f-3d06-4eb7-b525-3c921e0a9878",
   "metadata": {},
   "source": [
    "#### 3. What are the top 20 words in the corpus by TFIDF mean, if you using the `sum` count method and `chapter` as the bag? Note, because of the greater number of bags, this will take longer to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "cedaa413-0755-4a10-9aac-f96186cc3d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.004327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.004150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosmopolitan</th>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pierre</th>\n",
       "      <td>0.003317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communion</th>\n",
       "      <td>0.003004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.002771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sailors</th>\n",
       "      <td>0.002668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.002620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothetical</th>\n",
       "      <td>0.002437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>0.002084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidential</th>\n",
       "      <td>0.002042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dream</th>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boon</th>\n",
       "      <td>0.001857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrs</th>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elephants</th>\n",
       "      <td>0.001731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whale</th>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>0.001696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquaintance</th>\n",
       "      <td>0.001690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf\n",
       "term_str              \n",
       "her           0.004327\n",
       "she           0.004150\n",
       "cosmopolitan  0.003485\n",
       "pierre        0.003317\n",
       "communion     0.003004\n",
       "i             0.002771\n",
       "sailors       0.002668\n",
       "you           0.002620\n",
       "hypothetical  0.002437\n",
       "mr            0.002084\n",
       "and           0.002054\n",
       "confidential  0.002042\n",
       "the           0.001972\n",
       "dream         0.001942\n",
       "boon          0.001857\n",
       "mrs           0.001747\n",
       "elephants     0.001731\n",
       "whale         0.001715\n",
       "thou          0.001696\n",
       "acquaintance  0.001690"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtcm_chap = gen_bow(TOKEN, 'CHAPS')\n",
    "tfidf_chap = gen_tfidf(dtcm_chap, TF_METHOD='sum')\n",
    "\n",
    "\n",
    "tfidf_mean = tfidf_chap.mean(axis=0)  # Mean TF-IDF per term\n",
    "top20_chap = pd.DataFrame(tfidf_mean.sort_values(ascending=False).head(20), columns = ['tfidf'])\n",
    "top20_chap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830fdd5f-5d04-4aad-b691-40782fbc6562",
   "metadata": {},
   "source": [
    "#### 4. Characterize the general difference between the words in Question 3 and those in Question 2 in terms of part-of-speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0893cfd9-7681-4280-84d9-e0f839bf5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = TOKEN.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)\n",
    "VOCAB['max_pos'] = TOKEN[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)\n",
    "VOCAB['max_pos_group'] = TOKEN[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "55e43d7c-f511-4960-96cc-149bb4aca876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elinor</th>\n",
       "      <td>0.033840</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pierre</th>\n",
       "      <td>0.030911</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vernon</th>\n",
       "      <td>0.025980</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marianne</th>\n",
       "      <td>0.021347</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emma</th>\n",
       "      <td>0.021164</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>darcy</th>\n",
       "      <td>0.019302</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reginald</th>\n",
       "      <td>0.018486</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babbalanja</th>\n",
       "      <td>0.018252</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catherine</th>\n",
       "      <td>0.018238</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frederica</th>\n",
       "      <td>0.017986</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crawford</th>\n",
       "      <td>0.017749</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fanny</th>\n",
       "      <td>0.017167</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elliot</th>\n",
       "      <td>0.017053</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weston</th>\n",
       "      <td>0.016591</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>0.015986</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>0.015428</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knightley</th>\n",
       "      <td>0.015184</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tilney</th>\n",
       "      <td>0.013815</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elton</th>\n",
       "      <td>0.013648</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bingley</th>\n",
       "      <td>0.013264</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tfidf max_pos\n",
       "term_str                    \n",
       "elinor      0.033840     NNP\n",
       "pierre      0.030911     NNP\n",
       "vernon      0.025980     NNP\n",
       "marianne    0.021347     NNP\n",
       "emma        0.021164     NNP\n",
       "darcy       0.019302     NNP\n",
       "reginald    0.018486     NNP\n",
       "babbalanja  0.018252     NNP\n",
       "catherine   0.018238     NNP\n",
       "frederica   0.017986     NNP\n",
       "crawford    0.017749     NNP\n",
       "fanny       0.017167     NNP\n",
       "elliot      0.017053     NNP\n",
       "weston      0.016591     NNP\n",
       "media       0.015986     NNP\n",
       "israel      0.015428     NNP\n",
       "knightley   0.015184     NNP\n",
       "tilney      0.013815     NNP\n",
       "elton       0.013648     NNP\n",
       "bingley     0.013264     NNP"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(top20_book, VOCAB[['max_pos']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "89b03908-5d1c-4ab8-bf25-dd34cf5ff8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.004327</td>\n",
       "      <td>PRP$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.004150</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosmopolitan</th>\n",
       "      <td>0.003485</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pierre</th>\n",
       "      <td>0.003317</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communion</th>\n",
       "      <td>0.003004</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.002771</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sailors</th>\n",
       "      <td>0.002668</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>you</th>\n",
       "      <td>0.002620</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypothetical</th>\n",
       "      <td>0.002437</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>0.002084</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.002054</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidential</th>\n",
       "      <td>0.002042</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.001972</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dream</th>\n",
       "      <td>0.001942</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boon</th>\n",
       "      <td>0.001857</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrs</th>\n",
       "      <td>0.001747</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elephants</th>\n",
       "      <td>0.001731</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whale</th>\n",
       "      <td>0.001715</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>0.001696</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquaintance</th>\n",
       "      <td>0.001690</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tfidf max_pos\n",
       "term_str                      \n",
       "her           0.004327    PRP$\n",
       "she           0.004150     PRP\n",
       "cosmopolitan  0.003485      NN\n",
       "pierre        0.003317     NNP\n",
       "communion     0.003004      NN\n",
       "i             0.002771     PRP\n",
       "sailors       0.002668     NNS\n",
       "you           0.002620     PRP\n",
       "hypothetical  0.002437     NNP\n",
       "mr            0.002084     NNP\n",
       "and           0.002054      CC\n",
       "confidential  0.002042      JJ\n",
       "the           0.001972      DT\n",
       "dream         0.001942      NN\n",
       "boon          0.001857      NN\n",
       "mrs           0.001747     NNP\n",
       "elephants     0.001731      NN\n",
       "whale         0.001715      NN\n",
       "thou          0.001696      NN\n",
       "acquaintance  0.001690      NN"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(top20_chap, VOCAB[['max_pos']], left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc148e93-ad90-490f-b90e-e95f3d39863c",
   "metadata": {},
   "source": [
    "The top 20 words when book is used as the bag are all proper nouns, whereas the top 20 words with chapter as the bag include multiple parts of speech, such as pronouns, nouns, adjectives, determiners and conjunctions. The top 20 words with chapter as the bag still largely consist of nouns, but we see other parts of speech here as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c752ef-9da1-4ae1-ad18-b5670853bb40",
   "metadata": {},
   "source": [
    "#### 5. Compute mean `TFIDF` for vocabularies conditioned on individual author, using *chapter* as the bag and `max` as the `TF` count method. Among the two authors, whose work has the most significant adjective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c863ee1a-3dce-49b7-8556-284dc76aaa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = LIB.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "cd1983dd-79ca-420b-a4a7-43dae74bac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_id</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15859</th>\n",
       "      <th>1</th>\n",
       "      <th>12</th>\n",
       "      <th>2</th>\n",
       "      <th>51</th>\n",
       "      <td>15859</td>\n",
       "      <td>('a', 'DT')</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2701</th>\n",
       "      <th>136</th>\n",
       "      <th>31</th>\n",
       "      <th>0</th>\n",
       "      <th>39</th>\n",
       "      <td>2701</td>\n",
       "      <td>('the\"', 'NN')</td>\n",
       "      <td>NN</td>\n",
       "      <td>the\"</td>\n",
       "      <td>the</td>\n",
       "      <td>NN</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>10</th>\n",
       "      <th>47</th>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>('could', 'MD')</td>\n",
       "      <td>MD</td>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>MD</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13720</th>\n",
       "      <th>57</th>\n",
       "      <th>4</th>\n",
       "      <th>2</th>\n",
       "      <th>29</th>\n",
       "      <td>13720</td>\n",
       "      <td>('universe.', 'NN')</td>\n",
       "      <td>NN</td>\n",
       "      <td>universe.</td>\n",
       "      <td>universe</td>\n",
       "      <td>NN</td>\n",
       "      <td>MELVILLE, HERMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <th>2</th>\n",
       "      <th>6</th>\n",
       "      <th>5</th>\n",
       "      <th>9</th>\n",
       "      <td>105</td>\n",
       "      <td>('a', 'DT')</td>\n",
       "      <td>DT</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>AUSTEN, JANE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             book_id            pos_tuple pos  \\\n",
       "book_id chap_id para_num sent_num token_num                                     \n",
       "15859   1       12       2        51           15859          ('a', 'DT')  DT   \n",
       "2701    136     31       0        39            2701       ('the\"', 'NN')  NN   \n",
       "105     10      47       1        1              105      ('could', 'MD')  MD   \n",
       "13720   57      4        2        29           13720  ('universe.', 'NN')  NN   \n",
       "105     2       6        5        9              105          ('a', 'DT')  DT   \n",
       "\n",
       "                                             token_str  term_str pos_group  \\\n",
       "book_id chap_id para_num sent_num token_num                                  \n",
       "15859   1       12       2        51                 a         a        DT   \n",
       "2701    136     31       0        39              the\"       the        NN   \n",
       "105     10      47       1        1              could     could        MD   \n",
       "13720   57      4        2        29         universe.  universe        NN   \n",
       "105     2       6        5        9                  a         a        DT   \n",
       "\n",
       "                                                       author  \n",
       "book_id chap_id para_num sent_num token_num                    \n",
       "15859   1       12       2        51         MELVILLE, HERMAN  \n",
       "2701    136     31       0        39         MELVILLE, HERMAN  \n",
       "105     10      47       1        1              AUSTEN, JANE  \n",
       "13720   57      4        2        29         MELVILLE, HERMAN  \n",
       "105     2       6        5        9              AUSTEN, JANE  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get author information\n",
    "TOKEN2 = TOKEN.merge(LIB[['book_id', 'author']], on='book_id', how='left').set_index(TOKEN.index)\n",
    "TOKEN2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "73716b32-49f4-4560-b4f3-b140a17ef6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_AUSTEN = TOKEN2[TOKEN2['author'] == 'AUSTEN, JANE'].drop('book_id',axis=1)\n",
    "TOKEN_HERMAN = TOKEN2[TOKEN2['author'] == 'MELVILLE, HERMAN'].drop('book_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "399498cc-b4db-4385-8bd7-b94b3a105330",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtcm_austen = gen_bow(TOKEN_AUSTEN, 'CHAPS')\n",
    "dtcm_herman = gen_bow(TOKEN_HERMAN, 'CHAPS')\n",
    "\n",
    "tfidf_austen = gen_tfidf(dtcm_austen, TF_METHOD='max')\n",
    "tfidf_herman = gen_tfidf(dtcm_herman, TF_METHOD='max')\n",
    "\n",
    "tfidf_austen_mean = tfidf_austen.mean(axis=0)\n",
    "tfidf_herman_mean = tfidf_herman.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "be9e6a9c-28e1-4085-bcbc-7e6c771aa6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_austen_mean = pd.DataFrame(tfidf_austen_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "03843f43-dc3f-44b9-b126-aec2fe71fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_herman_mean = pd.DataFrame(tfidf_herman_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "be1a1c53-49be-460e-8c6f-0e001a6ddd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm a little confused about what the question is looking for, but I'm going to \n",
    "##  compare mean tfidfs for adjectives only across authors, find the most\n",
    "##  significant adjective for each, and then report which author's most significant\n",
    "##  adjective is more significant. I'll also average all tfidfs across adjectives\n",
    "##  and report which author has a higher average adjective signifiance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "f42fe8c0-0e8a-4a0f-b643-cbad6e172ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge vocab with each author's tfidf table\n",
    "tfidf_vocab_austen = tfidf_austen_mean.merge(VOCAB[['max_pos']], left_index=True, right_index=True)\n",
    "tfidf_vocab_herman = tfidf_herman_mean.merge(VOCAB[['max_pos']], left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "bf1977d3-9cfa-4a82-bcf8-2d6f554eecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter so that it's only adjectives\n",
    "tfidf_vocab_austen_adj = tfidf_vocab_austen[tfidf_vocab_austen['max_pos'].isin(['JJ', 'JJR', 'JJS'])]\n",
    "tfidf_vocab_herman_adj = tfidf_vocab_herman[tfidf_vocab_herman['max_pos'].isin(['JJ', 'JJR', 'JJS'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "62653e25-d4c9-48e9-9727-f65e4398396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vocab_austen_adj.columns = ['tfidf', 'max_pos']\n",
    "tfidf_vocab_herman_adj.columns = ['tfidf', 'max_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "9dba4615-221f-4702-b45e-8cdc2810190c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>0.013167</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dear</th>\n",
       "      <td>0.012992</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>0.012213</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper</th>\n",
       "      <td>0.011347</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.011327</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf max_pos\n",
       "term_str                  \n",
       "sure      0.013167      JJ\n",
       "dear      0.012992      JJ\n",
       "poor      0.012213      JJ\n",
       "upper     0.011347      JJ\n",
       "old       0.011327      JJ"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most significant adjective for each author\n",
    "tfidf_vocab_austen_adj.sort_values(by='tfidf',ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "73680378-7271-4e23-962a-62dbf13d814d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "      <th>max_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>0.028653</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.021042</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ugh</th>\n",
       "      <td>0.015733</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.014585</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>0.014173</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tfidf max_pos\n",
       "term_str                  \n",
       "thy       0.028653      JJ\n",
       "old       0.021042      JJ\n",
       "ugh       0.015733      JJ\n",
       "little    0.014585      JJ\n",
       "good      0.014173      JJ"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vocab_herman_adj.sort_values(by='tfidf',ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f475309-1d62-431e-8931-a25d64955c6b",
   "metadata": {},
   "source": [
    "According to this method, Herman's most significant adjective is more significant than Austen's most significant adjective. Thus, the most significant adjective across these works (conditional on author) is 'thy.' Let's try the other method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e261d401-7caa-40fe-8e22-25d411207dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015357254418560016"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vocab_austen_adj.tfidf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "f81fec3a-4373-450e-8581-e6bea76ed2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007973502092473931"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vocab_herman_adj.tfidf.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfeb8cf-b5c3-4878-b80e-b11e25563cfd",
   "metadata": {},
   "source": [
    "According to this method, Herman's adjectives have lower tfidfs, on average, than Austen's adjectives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
